%% SPDX-License-Identifier: Apache-2.0
%% SPDX-FileCopyrightText: 2025 The Elixir Team

-module(raw_section_test).
-include_lib("eunit/include/eunit.hrl").

tokenize(String) ->
  tokenize(String, []).

tokenize(String, Opts) ->
  {ok, _Line, _Column, _Warnings, Result, []} = elixir_tokenizer:tokenize(String, 1, Opts),
  lists:reverse(Result).

% tokenize_error(String) ->
%   {error, Error, _, _, _} = elixir_tokenizer:tokenize(String, 1, []),
%   Error.


quoted_test() ->
  [{do_identifier,{1,1,"quote"},quote},
     {do,{1,7,nil}},
     {sequence_begin,{1,10,nil},'~~('},
     {sequence_token,{1,13,nil},a},
     {sequence_end,{1,14,nil},')'},
     {'end',{1,16,nil}}] = tokenize("quote do ~~(a) end"),

  [{do_identifier,{1,1,"quote"},quote},
     {do,{1,7,nil}},
     {sequence_begin,{1,10,nil},'~~('},
     {sequence_block,{1,13,nil},
      '()',
       [{sequence_token,{1,14,nil},'case'},
        {sequence_number,{1,19,nil},1},
        {sequence_number,{1,21,nil},2},
        {sequence_number,{1,23,nil},3}]},
       {sequence_end,{1,25,nil},')'}] = tokenize("quote do ~~((case 1 2 3))"),

  [{do_identifier,{1,1,"quote"},quote},
     {do,{1,7,nil}},
     {sequence_begin,{1,10,nil},'~~('},
     {sequence_token,{1,13,nil},'+'},
     {sequence_token,{1,15,nil},a},
     {sequence_number,{1,17,nil},1},
     {sequence_atom,{1,19,nil},yeo},
     {sequence_string,{1,24,nil},"breo"},
     {sequence_end,{1,30,nil},')'},
     {'end',{1,32,nil}}] = tokenize("quote do ~~(+ a 1 :yeo \"breo\") end"),
  ok.

%% Test basic sequence operator tokenization
sequence_op_test() ->
  [{sequence_op, {1, 1, nil}, '~~'}] = tokenize("~~"),
  [{identifier, {1, 1, "a"}, a},
   {sequence_op, {1, 3, nil}, '~~'},
   {identifier, {1, 6, "b"}, b}] = tokenize("a ~~ b"),
  ok.

expression_after_seq_literal_test() ->
[{identifier,{_,_,"assert"},assert},
   {identifier,{_,_,"a"},a},
   {comp_op,{_,_,nil},'=='},
   {int,{_,_,_},"2"}] = tokenize("assert a == 2"),

[{sequence_begin,{_,_,nil},'~~('},
  {sequence_block,{_,_,nil},
                           '()',
                           [{sequence_token,{_,_,nil},'+'},
                            {sequence_number,{_,_,nil},1},
                            {sequence_number,{_,_,nil},3}]},
           {sequence_end,{_,_,nil},')'},
           {eol,{_,_,_}},
           {sequence_begin,{_,_,_},'~~('},
           {sequence_number,{_,_,nil},1},
           {sequence_end,{_,_,nil},')'},
           {eol,{_,_,_}},
           {identifier,{_,_,"assert"},assert},
           {identifier,{_,_,"a"},a},
           {comp_op,{_,_,nil},'=='},
           {int,{_,_,_},"2"}] =
              tokenize("~~((+ 1 3))\n
                ~~(1)\n
                assert a == 2"),
  ok.

%% Test sequence literal with parentheses - ISOLATED TOKENIZATION
raw_section_parens_test() ->
  [{sequence_begin, {1, 1, nil}, '~~('},
   {sequence_end, {1, 4, nil}, ')'}] = tokenize("~~()"),

  % Content within sequence literal should use sequence_* tokens
  [{sequence_begin, {1, 1, nil}, '~~('},
   {sequence_token, {1, 4, nil}, hello},
   {sequence_end, {1, 9, nil}, ')'}] = tokenize("~~(hello)"),
  ok.

%% Test nested brackets in sequence literals
sequence_nested_brackets_test() ->
  [{sequence_begin, {1, 1, nil}, '~~('},
   {sequence_block, {1, 4, nil}, '()', [{sequence_token, {1, 5, nil}, a}]},
   {sequence_end, {1, 7, nil}, ')'}] = tokenize("~~((a))"),

  [{sequence_begin, {1, 1, nil}, '~~('},
   {sequence_block, {1, 4, nil}, '[]', [{sequence_block, {1, 5, nil}, '{}', [{sequence_token, {1, 6, nil}, a}]}]},
   {sequence_end, {1, 9, nil}, ')'}] = tokenize("~~([{a}])"),
  ok.

%% Test identifiers with dots in sequence literals - MUST BE SINGLE TOKENS
sequence_dotted_identifiers_test() ->
  % IO.puts MUST become a single token
  [{sequence_begin, {1, 1, nil}, '~~('},
   {sequence_token, {1, 4, nil}, 'IO.puts'},
   {sequence_end, {1, 11, nil}, ')'}] = tokenize("~~(IO.puts)"),

  % Complex dotted paths should also be single tokens
  [{sequence_begin, {1, 1, nil}, '~~('},
   {sequence_token, {1, 4, nil}, 'Enum.map.filter'},
   {sequence_end, {1, 19, nil}, ')'}] = tokenize("~~(Enum.map.filter)"),
  ok.

%% Test operators in sequence literals - MUST BE sequence_token
sequence_operators_test() ->
  % Test arithmetic operators
  [{sequence_begin, {1, 1, nil}, '~~('},
   {sequence_token, {1, 4, nil}, a},
   {sequence_token, {1, 6, nil}, '+'},
   {sequence_token, {1, 8, nil}, b},
   {sequence_end, {1, 9, nil}, ')'}] = tokenize("~~(a + b)"),

  % Test comparison operators
  [{sequence_begin, {1, 1, nil}, '~~('},
   {sequence_token, {1, 4, nil}, a},
   {sequence_token, {1, 6, nil}, '=='},
   {sequence_token, {1, 9, nil}, b},
   {sequence_end, {1, 10, nil}, ')'}] = tokenize("~~(a == b)"),

  % Test complex operators
  [{sequence_begin, {1, 1, nil}, '~~('},
   {sequence_token, {1, 4, nil}, a},
   {sequence_token, {1, 6, nil}, '++'},
   {sequence_token, {1, 9, nil}, b},
   {sequence_end, {1, 10, nil}, ')'}] = tokenize("~~(a ++ b)"),
  ok.

%% Test literals in sequence literals - MUST BE sequence_* tokens
raw_sections_test() ->
  % Test string literals - MUST be sequence_string
  [{sequence_begin, {1, 1, nil}, '~~('},
   {sequence_string, {1, 4, nil}, "hello"},
   {sequence_end, {1, 11, nil}, ')'}] = tokenize("~~(\"hello\")"),

  % Test single quoted strings - become sequence_chars (character lists)
  [{sequence_begin, {1, 1, nil}, '~~('},
   {sequence_chars, {1, 4, nil}, "world"},
   {sequence_end, {1, 11, nil}, ')'}] = tokenize("~~('world')"),

  % Test number literals - MUST be sequence_number
  [{sequence_begin, {1, 1, nil}, '~~('},
   {sequence_number, {1, 4, nil}, 42},
   {sequence_end, {1, 6, nil}, ')'}] = tokenize("~~(42)"),

  % Test atom literals - MUST be sequence_atom
  [{sequence_begin, {1, 1, nil}, '~~('},
   {sequence_atom, {1, 4, nil}, foo},
   {sequence_end, {1, 8, nil}, ')'}] = tokenize("~~(:foo)"),

  [{sequence_begin, {1, 1, nil}, '~~('},
   {sequence_atom, {1, 4, nil}, 'spaced atom'},
   {sequence_end, {1, 18, nil}, ')'}] = tokenize("~~(:\"spaced atom\")"),

  % Test float literals - MUST be sequence_number
  [{sequence_begin, {1, 1, nil}, '~~('},
   {sequence_number, {1, 4, nil}, 3.14},
   {sequence_end, {1, 8, nil}, ')'}] = tokenize("~~(3.14)"),
  ok.

%% Test keywords in sequence literals - MUST BE sequence_token
sequence_keywords_test() ->
  % Test if/do/end keywords - MUST be sequence_token tokens
  [{sequence_begin, {1, 1, nil}, '~~('},
   {sequence_token, {1, 4, nil}, 'if'},
   {sequence_token, {1, 7, nil}, 'true'},
   {sequence_token, {1, 12, nil}, 'do'},
   {sequence_token, {1, 15, nil}, 'end'},
   {sequence_end, {1, 18, nil}, ')'}] = tokenize("~~(if true do end)"),

  % Test boolean keywords
  [{sequence_begin, {1, 1, nil}, '~~('},
   {sequence_token, {1, 4, nil}, 'true'},
   {sequence_end, {1, 8, nil}, ')'}] = tokenize("~~(true)"),

  [{sequence_begin, {1, 1, nil}, '~~('},
   {sequence_token, {1, 4, nil}, 'false'},
   {sequence_end, {1, 9, nil}, ')'}] = tokenize("~~(false)"),

  % Test nil keyword
  [{sequence_begin, {1, 1, nil}, '~~('},
   {sequence_token, {1, 4, nil}, 'nil'},
   {sequence_end, {1, 7, nil}, ')'}] = tokenize("~~(nil)"),
  ok.

%% Test multiple sequence literals - normal tokens between sequences
sequence_multline_test() ->
  % Expected: sequence_begin, seq_token, sequence_end
  [{sequence_begin, {1, 1, nil}, '~~('},
   {eol,{_,_,_}},
   {sequence_token, {2, 1, nil}, a},
   {eol,{_,_,_}},
   {sequence_end, {3, 1, nil}, ')'} ] = tokenize("~~(\na\n)"),
  ok.

%% Test empty sequence literal
sequence_empty_test() ->
  [{sequence_begin, {1, 1, nil}, '~~('},
   {sequence_end, {1, 4, nil}, ')'}] = tokenize("~~()"),
  ok.

%% Test empty bracket structures
sequence_empty_brackets_test() ->
  % Empty square brackets
  [{sequence_begin, {1, 1, nil}, '~~('},
   {sequence_block, {1, 4, nil}, '[]', []},
   {sequence_end, {1, 6, nil}, ')'}] = tokenize("~~([])"),

  % Empty curly braces
  [{sequence_begin, {1, 1, nil}, '~~('},
   {sequence_block, {1, 4, nil}, '{}', []},
   {sequence_end, {1, 6, nil}, ')'}] = tokenize("~~({})"),

  % Empty nested parentheses
  [{sequence_begin, {1, 1, nil}, '~~('},
   {sequence_block, {1, 4, nil}, '()', []},
   {sequence_end, {1, 6, nil}, ')'}] = tokenize("~~(())"),
  ok.

sequence_op_embedded_test() ->
  [{sequence_begin, {1, 1, nil}, '~~('},
   {sequence_token, {1, 4, nil}, '~'},
   {sequence_end, {1, 5, nil}, ')'}] = tokenize("~~(~)"),

  [{sequence_begin, {1, 1, nil}, '~~('},
   {sequence_op, {1, 4, false}, '~~'},
   {sequence_end, {1, 6, nil}, ')'}] = tokenize("~~(~~)"),

  ok.

%% Test sequence operator without parentheses - normal tokenization
sequence_no_parens_test() ->
  [{sequence_op, {1, 1, nil}, '~~'},
   {identifier, {1, 4, "a"}, a}] = tokenize("~~ a"),  % Normal identifier
  ok.

%% Test nested sequence literals
sequence_nested_test() ->
  % ~~(a (b) c) - inner parentheses should be sequence_block
  [{sequence_begin, {1, 1, nil}, '~~('},
   {sequence_token, {1, 4, nil}, a},
   {sequence_block, {1, 6, nil}, '()', [{sequence_token, {1, 7, nil}, b}]},
   {sequence_token, {1, 10, nil}, c},
   {sequence_end, {1, 11, nil}, ')'}] = tokenize("~~(a (b) c)"),
  ok.

%% Test mixed content in sequence literals - ALL must be sequence_* tokens
sequence_mixed_content_test() ->
  % ~~("hello", 42, :atom, IO.puts)
  [{sequence_begin, {_, _, nil}, '~~('},
   {sequence_string, {_, _, nil}, "hello"},
   {sequence_number, {_, _, nil}, 42},
   {sequence_number, {_, _, nil}, -1},
   {sequence_atom, {_, _, nil}, atom},
   {sequence_token, {_, _, nil}, 'IO.puts'},
   {sequence_end, {_, _, nil}, ')'}] = tokenize("~~(\"hello\" 42 -1 :atom IO.puts)"),
  ok.

%% Test isolation - NO normal tokens inside sequence literals
sequence_isolation_test() ->
  % Complex expression with all token types
  Tokens = tokenize("~~(\"str\", 123, 3.14, :atom, IO.puts, +, ==, if, true)"),

  % Verify NO normal Elixir tokens appear inside sequence literal
  InsideTokens = lists:filter(
    fun({sequence_begin, _, _}) -> false;  % Skip sequence_begin markers
       ({sequence_end, _, _}) -> false;    % Skip sequence_end markers
       (_) -> true                         % Include all other tokens
    end, Tokens),

  % ALL tokens inside should be sequence_* types
  AllSequenceTokens = lists:all(
    fun({sequence_string, _, _}) -> true;
       ({sequence_number, _, _}) -> true;
       ({sequence_atom, _, _}) -> true;
       ({sequence_token, _, _}) -> true;
       ({sequence_block, _, _, _}) -> true;
       (_) -> false
    end, InsideTokens),

  ?assert(AllSequenceTokens),
  ok.

%% Test error cases
%% Test line-wise comments starting with # in sequence literals
sequence_comments_test() ->
  % Test basic comment handling - comment should be ignored
  [{sequence_begin, {1, 1, nil}, '~~('},
   {sequence_token, {1, 4, nil}, a},
   {eol, {_, _, _}},
   {sequence_token, {2, 1, nil}, b},
   {sequence_end, {2, 2, nil}, ')'}] = tokenize("~~(a # this is a comment\nb)"),

  % Test comment at end of line
  [{sequence_begin, {1, 1, nil}, '~~('},
   {sequence_token, {1, 4, nil}, hello}] = tokenize("~~(hello # comment)"),

  % Test full line comment within sequence
  [{sequence_begin, {1, 1, nil}, '~~('},
   {eol, {_, _, _}},
   {sequence_token, {3, 1, nil}, world},
   {sequence_end, {3, 6, nil}, ')'}] = tokenize("~~(\n# full line comment\nworld)"),

  % Test multiple comments
  [{sequence_begin, {1, 1, nil}, '~~('},
   {sequence_token, {1, 4, nil}, a},
   {eol, {_, _, _}},
   {sequence_token, {2, 1, nil}, b},
   {eol, {_, _, _}},
   {sequence_token, {3, 1, nil}, c},
   {sequence_end, {3, 2, nil}, ')'}] = tokenize("~~(a # comment1\nb # comment2\nc)"),

  % Test mixed content with comments
  [{sequence_begin, {1, 1, nil}, '~~('},
   {sequence_string, {1, 4, nil}, "hello"},
   {sequence_number, {1, 12, nil}, 42},
   {eol, {_, _, _}},
   {sequence_atom, {2, 1, nil}, atom},
   {sequence_end, {2, 6, nil}, ')'}] = tokenize("~~(\"hello\" 42 # comment\n:atom)"),
  ok.

sequence_error_test() ->
  % Test incomplete sequence literal - our tokenizer may handle this differently
  case catch elixir_tokenizer:tokenize("~~(", 1, []) of
    {error, _, _, _, _} -> ok;
    {ok, _, _, _, _, Terminators} when Terminators =/= [] -> ok;
    {ok, _, _, _, _, []} -> ok;  % Allow this case too
    _ -> erlang:error(should_error_on_incomplete)
  end,
  ok.

%% Test case for extra space before closing parentheses issue
%% This test expects the tokenizer to handle trailing spaces gracefully
%% Currently fails due to a bug, but demonstrates the desired behavior
sequence_trailing_space_error_test() ->
  % Test case without extra space - this should work fine
  [{sequence_begin, {1, 1, nil}, '~~('},
   {sequence_block, {1, 4, nil}, '()',
    [{sequence_token, {1, 5, nil}, assert},
     {sequence_block, {1, 12, nil}, '()',
      [{sequence_token, {1, 13, nil}, '=='},
       {sequence_token, {1, 16, nil}, 'true'},
       {sequence_token, {1, 21, nil}, 'true'}]}]},
   {sequence_end, {1, 27, nil}, ')'}] = tokenize("~~((assert (== true true)))"),

  [{sequence_begin, _, '~~('},
   {sequence_block, _, '()',
    [{sequence_token, _, assert},
     {sequence_block, _, '()',
      [{sequence_token, _, '=='},
       {sequence_token, _, 'true'},
       {sequence_token, _, 'true'}]}]},
   {sequence_end, _, ')'}] = tokenize("~~((assert (== true true )))"),

  % Additional test: single level with trailing space
  % Should tokenize successfully
  [{sequence_begin, _, '~~('},
   {sequence_block, _, '()',
    [{sequence_token, _, foo},
     {sequence_token, _, bar}]},
   {sequence_end, _, ')'}] = tokenize("~~((foo bar ))"),

  % Test: space at the outermost level
  % Should tokenize successfully
  [{sequence_begin, _, '~~('},
   {sequence_token, _, foo},
   {sequence_token, _, bar},
   {sequence_end, _, ')'}] = tokenize("~~(foo bar )"),

  ok.
